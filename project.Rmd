---
title: "An Analysis of Violent Gun Crime in New York City from 2006"
author: "Zach Greenleaf"
date: "`r Sys.Date()`"
output: pdf_document
---

Violent gun crime is currently a hot topic in the debate over responsible gun control laws.
Here, we start with data reported by the city of New York comprised of every crime involving
a firearm where a police report was filed.  In this report, we we attempt to see if we can create a "Quick Response" system to predict the general location of a crime based on incomplete information.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We start by importing the required libraries.

```{r, message=FALSE}
### Uncomment the line below to install packages

# install.packages('tidyverse', 'lubridate', 'ggmap', 'fastDummies', 'dbplyr', 'corrplot', 'missMDA', 'RANN', 'e1071', 'superml', 'ranger')
library(tidyverse)
library(lubridate)
library(ggmap)
library(fastDummies)
library(dbplyr)
library(corrplot)
library(missMDA)
library(RANN)
library(e1071)
library(superml)
library(ranger)
```
# Section 1: Data Preprocessing

## 1.1: Import Data
We download the data from the web server.  A static view of the data is provided (current as
of the time of writing) in case the web server cannot be accessed.
```{r, message=FALSE}
#read csv file into dataframe
#url <- "./NYPD_Shooting_Incident_Data__Historic_.csv"
url <- "https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD"
df <- read_csv(url)
```

## 1.2: Data Cleaning
A cursory view of the data reveals a few issues.  We have improper data types, missing values,
and some features that don't hold meaning for us.
```{r}
print(head(df))
```
We will add a combined Date and Time column, to enable chronological sorting, and
fix the column data types.
```{r}
#add column OCCUR_DATETIME
df$OCCUR_DATETIME <- mdy_hms(paste(df$OCCUR_DATE, df$OCCUR_TIME))
df <- df %>% relocate(OCCUR_DATETIME, .after = OCCUR_TIME)
```

```{r}
#Fix data types
df$INCIDENT_KEY <- as.integer(df$INCIDENT_KEY)
df$OCCUR_DATE <- mdy(df$OCCUR_DATE)
df$OCCUR_TIME <- hms(df$OCCUR_TIME)
df$PRECINCT <- as.integer(df$PRECINCT)
df$JURISDICTION_CODE <- as.integer(df$JURISDICTION_CODE)
df$STATISTICAL_MURDER_FLAG <- as.integer(df$STATISTICAL_MURDER_FLAG)
df$BORO <- as.factor(df$BORO)
```

```{r}
head(df)
```
\newpage
### Missing Data
Now lets have a look at the missing data; there are a few columns that seem to
account for the majority.\
JURISDICTION\_CODE has only 2 missing values.  The value "0" occurs 83.3% of time in
this categorical value (corresponding to "patrol" according to the meta data).
It makes sense to do a simple mode imputation of these points.\
LOCATION\_DESC has a large number of missing values,
and it's values are split into many categories, many of which are too ambiguous or
too specific for a general analysis.  If the data were more complete, this might be worth
imputing the missing values and trying to use.\
PERP\_AGE\_GROUP, PERP\_SEX, PERP\_RACE are all interesting features.  We presume the cause of
the majority of the missing data is that the perpetrator was simply not caught.
This is supported by the fact that the majority of NA values co-occur in those columns.
For these reasons, we do not think that it would be useful to impute the values for these data.  Instead, we propose that the rows or the columns be dropped.  We chose to drop the
columns in order to maintain our sample size.  Dropping the rows could also
be worth investigating, but we have not done so.

```{r}
#count null values in each column
for(i in colnames(df))
{
    print(paste("nulls in ", i,"=", sum(is.na(df[[i]])),",", round(100*sum(is.na(df[[i]]))/nrow(df)), "%"))
}
```

```{r}
table(df$JURISDICTION_CODE)
```
```{r}
df$JURISDICTION_CODE[is.na(df$JURISDICTION_CODE)] <- 0
```

```{r}
unique(df$LOCATION_DESC)
```
```{r}
# Mean count of NA per row where PERP_SEX = NA
df_perp_na <-subset(df,is.na(PERP_SEX))

sum(is.na(df_perp_na))/nrow(df_perp_na)
```

```{r}
#remove columns with null values
df <- df[,colSums(is.na(df))==0]

```

### Pruning Co-linear and Low Information Features

There are a few features of this data that don't carry meaningful information for a predictive model.  According to the metadata, INCIDENT\_KEY is a randomly generated number, used to index the event.  Since a R data frame has an implicit index, this value can be safely dropped.\
X\_COORD\_CD and Y\_COORD\_CD columns names seem similar to Longitude" and Latitude, and a correlation analysis shows that both pairs of (X\_COORD\_CD, Longitude) and (Y\_COORD\_CD, Latitude) are 100% correlated.  This indicates redundant information, so we can drop one feature from each pair.  We chose to drop X\_COORD\_CD and Y\_COORD\_CD, simply for ease of use.  The metadata indicates that these values are midblock coordinates for the New York State Plane Coordinate System, and therefore might not be as universally applicable as Lat and Long.\
Lon\_Lat does not appear in the meta data, but it appears to be a concatenated string composed of the the Latitude and Longitude.  This redundant data can be disposed of.\


```{r}
cor_cols <- c('X_COORD_CD', 'Y_COORD_CD', 'Latitude', 'Longitude')

correlation <- cor(df[cor_cols])
corrplot(correlation, tl.cex=.6, method = "number")
```
```{r}
#drop redundant and meaningless data
df <- subset(df, select=-c(INCIDENT_KEY, X_COORD_CD, Y_COORD_CD, Lon_Lat))
```

### A Closer Look at the Categorical Data

Now that our data is a little more clean and orderly, we can have a closer look at our actual data.\
First, we check the unique values of the categorical data, and find that three columns, VIC_AGE_GROUP, 
VIC_SEX, and VIC_RACE, have some values coded as "UNKNOWN" or "U".  Looking a little closer, we can see that these missing data only effect about 100 observations.  Because less than 0.5% of the data is missing, We are going to discard those rows.


```{r}
exclude = c("OCCUR_DATE", "OCCUR_TIME", "OCCUR_DATETIME", "Latitude", "Longitude", "STATISTICAL_MURDER_FLAG", "BORO", "PRECINCT", "JURISDICTION_CODE")
for(i in colnames(df[, !names(df) %in% exclude])){
  print(unique(df[,i]))
}
```
```{r}
check_nonstandard_null <- function(){
  print(paste("Num of UNKNOWN in VIC_AGE_GROUP:",sum(df$VIC_AGE_GROUP == "UNKNOWN")))
  print(paste("Num of UNKNOWN in VIC_RACE:",sum(df$VIC_RACE == "UNKNOWN")))
  print(paste("Num of U in VIC_SEX:",sum(df$VIC_SEX == "U")))
}
check_nonstandard_null()
```


```{r}
replace_value <- function(fdf, coln, orig, repl){
  for(i in 1:nrow(fdf)){
    if(fdf[i, coln] == orig){
      fdf[i, coln] <- repl
    }
  }
  return(fdf)
}

df <- replace_value(df,"VIC_AGE_GROUP", "UNKNOWN", NA)
df <- replace_value(df,"VIC_RACE", "UNKNOWN", NA)
df <- replace_value(df,"VIC_SEX", "U", NA)

check_nonstandard_null()

```

```{r}
df <- na.omit(df)

```

```{r}
#sort by OCCUR_DATETIME
df <- df[order(df$OCCUR_DATETIME, decreasing = FALSE),]
```

```{r}
print(df)
```
# Section 2: Exploratory Data Analysis

We can see below that the number of incidents seems to decrease in the wintertime.  Some possible causes could be the temperature outside, or perhaps another complicating factor that occurs on a yearly cycle.  Another interesting feature is the data is the apparent dip in activity in the years from 2018-2020, followed by a sharp increase in cases in 2020 and 2021.  The interval of 2020-21 corresponds with the onset of COVID-19 in New York City (March 2020 - Current).  It could be interesting to incorporate temperature and COVID data into this model, but we have not done so.

```{r}
#plot OCCUR_DATE as a histogram
v=seq(ymd('2007-01-01'),ymd('2021-01-01'), by = 'years')

hist(df$OCCUR_DATE, breaks = "months")
abline(v=v,col="light blue",lwd=1)
```

To get a feel for where the crimes are happening, we plotted the physical location onto a series of heat maps.  We show data for: All crimes, murders, and non-murders.  Separating the heat maps into murders and non-murders did not yield valuable information, as there doesn't seem to be an obvious pattern to the difference.\
The overall map does show us that most events (69.4%) are happening in Brooklyn (40.5%) and the Bronx (28.9%).  In the case of Brooklyn, this is not neccisarily surprising.  According to data from https://www.citypopulation.de/en/usa/newyorkcity/ the population of Brooklyn accounts for 31.1% of the population of the city.  The Bronx, however, only comprises 16.7% of the city's population, and so may have a higher per capita rate than the rest of the city.

```{r}
plot_data_on_map <- function(df_map, title){

  height <- max(df_map$Latitude) - min(df_map$Latitude)
  width <- max(df_map$Longitude) - min(df_map$Longitude)

  nyc_borders <- c(bottom  = min(df_map$Latitude),
                     top     = max(df_map$Latitude),
                     left    = min(df_map$Longitude),
                     right   = max(df_map$Longitude))
  map <- get_stamenmap(nyc_borders, zoom = 10, maptype = "terrain")

  ggmap(map) + ggtitle(title) + geom_bin2d(data=df_map,bins=150, aes(x=Longitude, y=Latitude), alpha=.8)
}
plot_data_on_map(df, "All Gun Crimes")
plot_data_on_map(df[df$STATISTICAL_MURDER_FLAG == TRUE,], "Murders")
plot_data_on_map(df[df$STATISTICAL_MURDER_FLAG == FALSE,], "Non-lethal Gun Crimes")
```

We plot the correlation matrix of the continuous variables, and we see that the only things that are strongly correlated are OCCUR_DATE and OCCUR_TIME with OCCUR_DATETIME.  This is obvious, since OCCUR_DATETIME is a concatenation of the other two.  We should drop either DATETIME, or both of the others.\
Additionally, precinct is negatively correlated with Latitude.  It is not surprising that the precinct is correlated with geographical location, as precincts are defined by their physical location.  Precinct number should either be treated categorically, or dropped entirely.
```{r}
df.num <- df %>% select(where(~ is.integer(.x) | is.double(.x)))
df.num <- transform(df.num, OCCUR_DATE=as.numeric(OCCUR_DATE),
                          OCCUR_TIME=as.numeric(OCCUR_TIME),
                          OCCUR_DATETIME=as.numeric(OCCUR_DATETIME))
correlation <- cor(df.num)
corrplot(correlation, tl.cex=.6, method = "number")


```

# Section 3: Modelling

We will try to predict, from this data, which BORO a crime is occurring in.
First, we prepare our dataset to use for modelling.  We remove OCCUR_DATETIME and PRECINT, with the justification from section 2.  We also convert OCCUR_DATE and OCCUR_TIME to numeric values, as required for our models, and convert our categorical data to factors.  Our modelling strategy will be to use a multiple linear regression to get a feel for the important factors in our data, then move on to more advanced models.  We have selected a support vector machine for its strong performance in high dimension, low density spaces.  We will also investigate a grid searched random forest.  Other options could be: a gradient boosted decision tree, or a neural network.
We also perform a train/test split on our data.  We will use a randomly selected 80% for training, and reserve the remaining 20% for validation.

```{r}
set.seed(42)
df_for_model <- subset(df, select=-c(OCCUR_DATETIME, PRECINCT, Latitude, Longitude))
df_for_model <- transform(df_for_model, OCCUR_DATE=as.numeric(OCCUR_DATE),
                          OCCUR_TIME=as.numeric(OCCUR_TIME))
```

```{r}

names <- c(3,4,5,6,7,8)
df_for_model[,names] <- lapply(df_for_model[,names] , factor)
glimpse(df_for_model)

#train test split
sample <- sample(c(TRUE, FALSE), nrow(df_for_model), replace=TRUE, prob=c(0.8,0.2))
train  <- df_for_model[sample, ]
test   <- df_for_model[!sample, ]
```
The results of the multiple linear regression show us a few things.  Values with any number of asterisks next to the row have a p-value of less than .05, and are therefore statistically significant.  It appears that most of our data is significant with the exception of race, time, and murder flag.

```{r}
model_glm = glm(BORO ~ ., family="binomial", data=train)
summary(model_glm)
```
The SVM classifier was run without parameter optimization, to investigate it's viability.  It appears that the model has more or less disregarded Staten Island, which makes some sense, because fewer than 3% of the data occurred there.  Manhattan is also strongly mis-classified, which is odd, because 12.7% of the data occurred there.  This model does not fit well in it's current state.

```{r}
classifier = svm(formula = BORO ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'sigmoid')
```

```{r}
y_pred = predict(classifier, newdata = test)

```

```{r}
conf_matrix = table(test[,3], y_pred)
conf_matrix
```
A grid searched random forest is attempted below.  The resulting model provides slightly worse than a random guess in it's current state, as evidenced by the AUC score.  This model is also not a good fit.
```{r}
parameters = list(max_features = c('sqrt', 'log'),
                 max_depth = seq(1,10,1))
rf <- RFTrainer$new(n_estimators = 100)
gst = GridSearchCV$new(rf, parameters, n_folds = 5, scoring = "auc")
```

```{r}
gst$fit(df_for_model,"BORO")

```
```{r}
gst$best_iteration()
```

# Section 4: Conclusions and areas of further study

Our most successful model was also the simplest: the logistic regression.  The other machine learning models described do not fit well.  This could be a result of either poor optimization, or not enough relevant data.  In order to improve their predictive accuracy, we might look to add to our data set.  Since we suspect that outdoor conditions might influence our data, it could be of interest to include meteorologic data.  Additionally, rates seemed to be decreasing until the onset of COVID-19.  It could make sense to include data such as number of cases per day etc.
We could also slice the data into smaller chunks for analysis.  Perhaps precinct would be a better dependent variable, or the data might become more clear when looked at individually by BORO. There are a lot of ares for improvement and further study, and this investigation could serve as a jumping off point.